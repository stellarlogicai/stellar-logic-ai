#!/usr/bin/env python3
"""
Stellar Logic AI - Reinforcement Learning Platform
Multi-agent RL, reward optimization, and autonomous decision-making
"""

import numpy as np
from typing import List, Dict, Tuple, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
import random
import math
import json
import time
from collections import defaultdict, deque
import gym
from gym import spaces

class RLAlgorithm(Enum):
    """Types of reinforcement learning algorithms"""
    Q_LEARNING = "q_learning"
    DEEP_Q_NETWORK = "deep_q_network"
    POLICY_GRADIENT = "policy_gradient"
    ACTOR_CRITIC = "actor_critic"
    PROXIMAL_POLICY_OPTIMIZATION = "proximal_policy_optimization"
    DEEP_DETERMINISTIC_POLICY_GRADIENT = "ddpg"
    MULTI_AGENT_DQN = "multi_agent_dqn"
    HIERARCHICAL_RL = "hierarchical_rl"

class EnvironmentType(Enum):
    """Types of RL environments"""
    GRID_WORLD = "grid_world"
    CART_POLE = "cart_pole"
    FINANCIAL_TRADING = "financial_trading"
    RESOURCE_ALLOCATION = "resource_allocation"
    GAME_STRATEGY = "game_strategy"
    AUTONOMOUS_NAVIGATION = "autonomous_navigation"

@dataclass
class RLAgent:
    """Represents a reinforcement learning agent"""
    agent_id: str
    algorithm: RLAlgorithm
    state_space: spaces.Space
    action_space: spaces.Space
    policy_network: Any
    value_network: Any
    experience_buffer: deque
    performance_history: List[float]
    hyperparameters: Dict[str, Any]
    training_episodes: int = 0
    total_reward: float = 0.0

@dataclass
class Experience:
    """Represents a single experience tuple"""
    state: np.ndarray
    action: int
    reward: float
    next_state: np.ndarray
    done: bool
    timestamp: float

@dataclass
class RLEnvironment:
    """Represents an RL environment"""
    env_id: str
    environment_type: EnvironmentType
    state_space: spaces.Space
    action_space: spaces.Space
    max_episodes: int
    max_steps_per_episode: int
    current_episode: int = 0
    total_steps: int = 0

class BaseRLAgent(ABC):
    """Base class for RL agents"""
    
    def __init__(self, agent_id: str, algorithm: RLAlgorithm, 
                 state_space: spaces.Space, action_space: spaces.Space):
        self.id = agent_id
        self.algorithm = algorithm
        self.state_space = state_space
        self.action_space = action_space
        self.experience_buffer = deque(maxlen=10000)
        self.performance_history = []
        self.training_episodes = 0
        self.total_reward = 0.0
        
    @abstractmethod
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """Select action based on current policy"""
        pass
    
    @abstractmethod
    def update_policy(self, experience: Experience) -> Dict[str, float]:
        """Update policy based on experience"""
        pass
    
    @abstractmethod
    def train_episode(self, environment: Any) -> Dict[str, Any]:
        """Train for one episode"""
        pass
    
    def add_experience(self, experience: Experience) -> None:
        """Add experience to buffer"""
        self.experience_buffer.append(experience)
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary"""
        if not self.performance_history:
            return {'status': 'no_training_history'}
        
        recent_performance = self.performance_history[-100:]  # Last 100 episodes
        
        return {
            'agent_id': self.id,
            'algorithm': self.algorithm.value,
            'total_episodes': self.training_episodes,
            'average_reward': np.mean(recent_performance),
            'max_reward': np.max(recent_performance),
            'min_reward': np.min(recent_performance),
            'recent_trend': 'improving' if len(recent_performance) > 10 and 
                           np.mean(recent_performance[-10:]) > np.mean(recent_performance[-20:-10]) 
                           else 'stable',
            'experience_buffer_size': len(self.experience_buffer)
        }

class QLearningAgent(BaseRLAgent):
    """Q-Learning algorithm implementation"""
    
    def __init__(self, agent_id: str, state_space: spaces.Space, action_space: spaces.Space,
                 learning_rate: float = 0.1, discount_factor: float = 0.95, 
                 epsilon: float = 0.1):
        super().__init__(agent_id, RLAlgorithm.Q_LEARNING, state_space, action_space)
        
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.q_table = defaultdict(lambda: defaultdict(float))
        
        # For discrete state spaces
        if isinstance(state_space, spaces.Discrete) and isinstance(action_space, spaces.Discrete):
            self.q_table = np.zeros((state_space.n, action_space.n))
        
        self.hyperparameters = {
            'learning_rate': learning_rate,
            'discount_factor': discount_factor,
            'epsilon': epsilon
        }
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """Select action using epsilon-greedy policy"""
        # Convert state to discrete representation
        state_key = self._discretize_state(state)
        
        if training and random.random() < self.epsilon:
            # Explore: random action
            if isinstance(self.action_space, spaces.Discrete):
                return random.randint(0, self.action_space.n - 1)
            else:
                return self.action_space.sample()
        else:
            # Exploit: best action
            if isinstance(self.action_space, spaces.Discrete):
                if isinstance(self.q_table, np.ndarray):
                    return np.argmax(self.q_table[state_key])
                else:
                    # Handle defaultdict case
                    q_values = self.q_table[state_key]
                    return max(q_values.keys(), key=lambda k: q_values[k]) if q_values else 0
            else:
                return self.action_space.sample()
    
    def _discretize_state(self, state: np.ndarray) -> int:
        """Convert continuous state to discrete"""
        if isinstance(self.state_space, spaces.Discrete):
            return int(state) if len(state.shape) == 0 else int(state[0])
        else:
            # Discretize continuous state space
            # Simple binning approach
            discretized = tuple(np.clip((state * 10).astype(int), 0, 99))
            return hash(discretized) % 1000  # Limit to 1000 discrete states
    
    def update_policy(self, experience: Experience) -> Dict[str, float]:
        """Update Q-table based on experience"""
        state_key = self._discretize_state(experience.state)
        next_state_key = self._discretize_state(experience.next_state)
        
        # Current Q-value
        current_q = self.q_table[state_key][experience.action] if isinstance(self.q_table, dict) else self.q_table[state_key, experience.action]
        
        # Maximum Q-value for next state
        if isinstance(self.q_table, dict):
            next_q_values = self.q_table[next_state_key]
            max_next_q = max(next_q_values.values()) if next_q_values else 0.0
        else:
            max_next_q = np.max(self.q_table[next_state_key]) if not experience.done else 0.0
        
        # Q-learning update rule
        new_q = current_q + self.learning_rate * (
            experience.reward + self.discount_factor * max_next_q - current_q
        )
        
        if isinstance(self.q_table, dict):
            self.q_table[state_key][experience.action] = new_q
        else:
            self.q_table[state_key, experience.action] = new_q
        
        return {
            'q_value_change': abs(new_q - current_q),
            'new_q_value': new_q,
            'learning_rate': self.learning_rate
        }
    
    def train_episode(self, environment: Any) -> Dict[str, Any]:
        """Train for one episode"""
        state = environment.reset()
        episode_reward = 0.0
        steps = 0
        losses = []
        
        while steps < environment.max_steps_per_episode:
            # Select action
            action = self.select_action(state, training=True)
            
            # Take step
            next_state, reward, done, info = environment.step(action)
            
            # Create experience
            experience = Experience(
                state=state,
                action=action,
                reward=reward,
                next_state=next_state,
                done=done,
                timestamp=time.time()
            )
            
            # Update policy
            update_result = self.update_policy(experience)
            losses.append(update_result['q_value_change'])
            
            # Add to buffer
            self.add_experience(experience)
            
            # Update state and reward
            state = next_state
            episode_reward += reward
            steps += 1
            
            if done:
                break
        
        # Update training metrics
        self.training_episodes += 1
        self.total_reward += episode_reward
        self.performance_history.append(episode_reward)
        
        # Decay epsilon
        self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return {
            'episode': self.training_episodes,
            'reward': episode_reward,
            'steps': steps,
            'epsilon': self.epsilon,
            'average_loss': np.mean(losses) if losses else 0.0,
            'training_success': True
        }

class DeepQNetworkAgent(BaseRLAgent):
    """Deep Q-Network implementation"""
    
    def __init__(self, agent_id: str, state_space: spaces.Space, action_space: spaces.Space,
                 hidden_layers: List[int] = [64, 64], learning_rate: float = 0.001,
                 discount_factor: float = 0.95, epsilon: float = 0.1):
        super().__init__(agent_id, RLAlgorithm.DEEP_Q_NETWORK, state_space, action_space)
        
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.hidden_layers = hidden_layers
        self.batch_size = 32
        self.target_update_freq = 100
        
        # Initialize neural networks
        self.q_network = self._create_q_network()
        self.target_network = self._create_q_network()
        self.update_count = 0
        
        self.hyperparameters = {
            'learning_rate': learning_rate,
            'discount_factor': discount_factor,
            'epsilon': epsilon,
            'batch_size': self.batch_size,
            'hidden_layers': hidden_layers
        }
    
    def _create_q_network(self) -> Dict[str, np.ndarray]:
        """Create Q-network weights"""
        network = {}
        
        # Input layer
        input_size = self._get_state_size()
        layer_sizes = [input_size] + self.hidden_layers + [self._get_action_size()]
        
        # Initialize weights for each layer
        for i in range(len(layer_sizes) - 1):
            network[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.1
            network[f'b{i}'] = np.zeros(layer_sizes[i+1])
        
        return network
    
    def _get_state_size(self) -> int:
        """Get state dimension"""
        if isinstance(self.state_space, spaces.Discrete):
            return self.state_space.n
        elif isinstance(self.state_space, spaces.Box):
            return np.prod(self.state_space.shape)
        else:
            return 10  # Default
    
    def _get_action_size(self) -> int:
        """Get action dimension"""
        if isinstance(self.action_space, spaces.Discrete):
            return self.action_space.n
        elif isinstance(self.action_space, spaces.Box):
            return np.prod(self.action_space.shape)
        else:
            return 4  # Default
    
    def _forward_pass(self, network: Dict[str, np.ndarray], state: np.ndarray) -> np.ndarray:
        """Forward pass through network"""
        x = state.flatten()
        
        layer_idx = 0
        while f'W{layer_idx}' in network:
            # Linear layer
            x = np.dot(x, network[f'W{layer_idx}']) + network[f'b{layer_idx}']
            
            # Activation (except for output layer)
            if f'W{layer_idx+1}' in network:
                x = np.maximum(0, x)  # ReLU
            
            layer_idx += 1
        
        return x
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """Select action using epsilon-greedy policy"""
        if training and random.random() < self.epsilon:
            # Explore
            if isinstance(self.action_space, spaces.Discrete):
                return random.randint(0, self.action_space.n - 1)
            else:
                return self.action_space.sample()
        else:
            # Exploit
            q_values = self._forward_pass(self.q_network, state)
            
            if isinstance(self.action_space, spaces.Discrete):
                return np.argmax(q_values)
            else:
                # For continuous action spaces, sample from distribution
                return q_values
    
    def update_policy(self, experience: Experience) -> Dict[str, float]:
        """Update Q-network using experience replay"""
        if len(self.experience_buffer) < self.batch_size:
            return {'error': 'insufficient_experience'}
        
        # Sample batch
        batch = random.sample(list(self.experience_buffer), self.batch_size)
        
        # Prepare batch data
        states = np.array([exp.state for exp in batch])
        actions = np.array([exp.action for exp in batch])
        rewards = np.array([exp.reward for exp in batch])
        next_states = np.array([exp.next_state for exp in batch])
        dones = np.array([exp.done for exp in batch])
        
        # Current Q-values
        current_q_values = []
        for state in states:
            q_vals = self._forward_pass(self.q_network, state)
            current_q_values.append(q_vals)
        
        # Target Q-values
        target_q_values = []
        for next_state, done in zip(next_states, dones):
            if done:
                target_q = 0.0
            else:
                next_q_vals = self._forward_pass(self.target_network, next_state)
                target_q = np.max(next_q_vals)
            target_q_values.append(target_q)
        
        # Compute loss and update weights (simplified)
        total_loss = 0.0
        for i, (state, action, reward, target_q) in enumerate(zip(states, actions, rewards, target_q_values)):
            current_q = current_q_values[i][action]
            target = reward + self.discount_factor * target_q
            loss = (current_q - target) ** 2
            total_loss += loss
            
            # Simple gradient update (simplified)
            self._simple_weight_update(state, action, target)
        
        # Update target network
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.target_network = {k: v.copy() for k, v in self.q_network.items()}
        
        return {
            'loss': total_loss / self.batch_size,
            'epsilon': self.epsilon,
            'update_count': self.update_count
        }
    
    def _simple_weight_update(self, state: np.ndarray, action: int, target: float) -> None:
        """Simple weight update (simplified gradient descent)"""
        # This is a very simplified version of backpropagation
        # In practice, you'd use proper gradient computation
        
        # Get current prediction
        current_q_values = self._forward_pass(self.q_network, state)
        current_q = current_q_values[action]
        
        # Compute error
        error = target - current_q
        
        # Simple weight update (just adjust the output layer)
        if 'W2' in self.q_network:  # Assuming at least 2 layers
            # Update output layer weights
            self.q_network['W2'][:, action] += self.learning_rate * error * 0.1
            self.q_network['b2'][action] += self.learning_rate * error * 0.1
    
    def train_episode(self, environment: Any) -> Dict[str, Any]:
        """Train for one episode"""
        state = environment.reset()
        episode_reward = 0.0
        steps = 0
        losses = []
        
        while steps < environment.max_steps_per_episode:
            # Select action
            action = self.select_action(state, training=True)
            
            # Take step
            next_state, reward, done, info = environment.step(action)
            
            # Create experience
            experience = Experience(
                state=state,
                action=action,
                reward=reward,
                next_state=next_state,
                done=done,
                timestamp=time.time()
            )
            
            # Add to buffer
            self.add_experience(experience)
            
            # Update policy (if enough experience)
            if len(self.experience_buffer) >= self.batch_size:
                update_result = self.update_policy(experience)
                if 'loss' in update_result:
                    losses.append(update_result['loss'])
            
            # Update state and reward
            state = next_state
            episode_reward += reward
            steps += 1
            
            if done:
                break
        
        # Update training metrics
        self.training_episodes += 1
        self.total_reward += episode_reward
        self.performance_history.append(episode_reward)
        
        # Decay epsilon
        self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return {
            'episode': self.training_episodes,
            'reward': episode_reward,
            'steps': steps,
            'epsilon': self.epsilon,
            'average_loss': np.mean(losses) if losses else 0.0,
            'buffer_size': len(self.experience_buffer),
            'training_success': True
        }

class MultiAgentEnvironment:
    """Multi-agent RL environment"""
    
    def __init__(self, env_id: str, num_agents: int, state_size: int, action_size: int):
        self.env_id = env_id
        self.num_agents = num_agents
        self.state_size = state_size
        self.action_size = action_size
        self.current_step = 0
        self.max_steps = 1000
        
    def reset(self) -> List[np.ndarray]:
        """Reset environment for all agents"""
        self.current_step = 0
        # Return initial states for all agents
        return [np.random.randn(self.state_size) for _ in range(self.num_agents)]
    
    def step(self, actions: List[int]) -> Tuple[List[np.ndarray], List[float], List[bool], Dict]:
        """Take step for all agents"""
        self.current_step += 1
        
        # Generate next states
        next_states = [np.random.randn(self.state_size) for _ in range(self.num_agents)]
        
        # Generate rewards (cooperative or competitive)
        rewards = []
        for i in range(self.num_agents):
            # Cooperative reward: all agents get same reward based on collective action
            collective_reward = np.sum(actions) / (self.num_agents * self.action_size)
            rewards.append(collective_reward + np.random.normal(0, 0.1))
        
        # Check if done
        done = self.current_step >= self.max_steps
        dones = [done] * self.num_agents
        
        info = {'step': self.current_step, 'collective_action': np.sum(actions)}
        
        return next_states, rewards, dones, info

class ReinforcementLearningPlatform:
    """Complete RL platform"""
    
    def __init__(self):
        self.agents = {}
        self.environments = {}
        self.training_sessions = {}
        self.performance_metrics = {}
        
    def create_agent(self, agent_id: str, algorithm: str, state_space: spaces.Space, 
                    action_space: spaces.Space, **kwargs) -> Dict[str, Any]:
        """Create an RL agent"""
        print(f"ğŸ¤– Creating RL Agent: {agent_id} ({algorithm})")
        
        try:
            algorithm_enum = RLAlgorithm(algorithm)
            
            if algorithm_enum == RLAlgorithm.Q_LEARNING:
                agent = QLearningAgent(agent_id, state_space, action_space, **kwargs)
            elif algorithm_enum == RLAlgorithm.DEEP_Q_NETWORK:
                agent = DeepQNetworkAgent(agent_id, state_space, action_space, **kwargs)
            else:
                return {'error': f'Unsupported algorithm: {algorithm}'}
            
            self.agents[agent_id] = agent
            
            return {
                'agent_id': agent_id,
                'algorithm': algorithm,
                'state_space': str(state_space),
                'action_space': str(action_space),
                'creation_success': True
            }
            
        except ValueError as e:
            return {'error': str(e)}
    
    def create_environment(self, env_id: str, environment_type: str, 
                          state_size: int, action_size: int, **kwargs) -> Dict[str, Any]:
        """Create an RL environment"""
        print(f"ğŸŒ Creating RL Environment: {env_id} ({environment_type})")
        
        try:
            env_type_enum = EnvironmentType(environment_type)
            
            if env_type_enum == EnvironmentType.RESOURCE_ALLOCATION:
                # Custom environment for resource allocation
                state_space = spaces.Box(low=-10, high=10, shape=(state_size,))
                action_space = spaces.Discrete(action_size)
                environment = RLEnvironment(
                    env_id=env_id,
                    environment_type=env_type_enum,
                    state_space=state_space,
                    action_space=action_space,
                    max_episodes=kwargs.get('max_episodes', 1000),
                    max_steps_per_episode=kwargs.get('max_steps', 100)
                )
            else:
                return {'error': f'Unsupported environment type: {environment_type}'}
            
            self.environments[env_id] = environment
            
            return {
                'env_id': env_id,
                'environment_type': environment_type,
                'state_space': str(state_space),
                'action_space': str(action_space),
                'creation_success': True
            }
            
        except ValueError as e:
            return {'error': str(e)}
    
    def train_agent(self, agent_id: str, env_id: str, num_episodes: int) -> Dict[str, Any]:
        """Train an agent in an environment"""
        if agent_id not in self.agents:
            return {'error': f'Agent {agent_id} not found'}
        
        if env_id not in self.environments:
            return {'error': f'Environment {env_id} not found'}
        
        agent = self.agents[agent_id]
        environment = self.environments[env_id]
        
        print(f"ğŸ‹ï¸ Training Agent {agent_id} for {num_episodes} episodes")
        
        training_results = []
        total_reward = 0.0
        
        for episode in range(num_episodes):
            # Create mock environment for training
            mock_env = MockEnvironment(environment)
            
            # Train one episode
            episode_result = agent.train_episode(mock_env)
            episode_result['episode_number'] = episode + 1
            training_results.append(episode_result)
            
            total_reward += episode_result['reward']
            
            # Print progress
            if (episode + 1) % 10 == 0:
                avg_reward = total_reward / (episode + 1)
                print(f"Episode {episode + 1}/{num_episodes}, Avg Reward: {avg_reward:.3f}")
        
        # Store training session
        session_id = f"session_{int(time.time())}"
        self.training_sessions[session_id] = {
            'agent_id': agent_id,
            'env_id': env_id,
            'training_results': training_results,
            'total_episodes': num_episodes,
            'average_reward': total_reward / num_episodes,
            'timestamp': time.time()
        }
        
        return {
            'session_id': session_id,
            'agent_id': agent_id,
            'env_id': env_id,
            'total_episodes': num_episodes,
            'average_reward': total_reward / num_episodes,
            'training_results': training_results,
            'training_success': True
        }
    
    def evaluate_agent(self, agent_id: str, env_id: str, num_episodes: int) -> Dict[str, Any]:
        """Evaluate an agent"""
        if agent_id not in self.agents:
            return {'error': f'Agent {agent_id} not found'}
        
        agent = self.agents[agent_id]
        mock_env = MockEnvironment(self.environments[env_id])
        
        evaluation_results = []
        total_reward = 0.0
        
        for episode in range(num_episodes):
            state = mock_env.reset()
            episode_reward = 0.0
            steps = 0
            
            while steps < mock_env.max_steps_per_episode:
                # Select action (no exploration)
                action = agent.select_action(state, training=False)
                next_state, reward, done, info = mock_env.step(action)
                
                episode_reward += reward
                state = next_state
                steps += 1
                
                if done:
                    break
            
            evaluation_results.append({
                'episode': episode + 1,
                'reward': episode_reward,
                'steps': steps
            })
            total_reward += episode_reward
        
        return {
            'agent_id': agent_id,
            'env_id': env_id,
            'evaluation_episodes': num_episodes,
            'average_reward': total_reward / num_episodes,
            'max_reward': max(result['reward'] for result in evaluation_results),
            'min_reward': min(result['reward'] for result in evaluation_results),
            'evaluation_results': evaluation_results,
            'evaluation_success': True
        }
    
    def get_platform_summary(self) -> Dict[str, Any]:
        """Get platform summary"""
        agent_summaries = {}
        for agent_id, agent in self.agents.items():
            agent_summaries[agent_id] = agent.get_performance_summary()
        
        return {
            'total_agents': len(self.agents),
            'total_environments': len(self.environments),
            'total_training_sessions': len(self.training_sessions),
            'agent_summaries': agent_summaries,
            'supported_algorithms': [algo.value for algo in RLAlgorithm],
            'supported_environments': [env.value for env in EnvironmentType]
        }

class MockEnvironment:
    """Mock environment for testing"""
    
    def __init__(self, rl_environment: RLEnvironment):
        self.env_id = rl_environment.env_id
        self.environment_type = rl_environment.environment_type
        self.state_space = rl_environment.state_space
        self.action_space = rl_environment.action_space
        self.max_steps_per_episode = rl_environment.max_steps_per_episode
        self.current_step = 0
        
    def reset(self) -> np.ndarray:
        """Reset environment"""
        self.current_step = 0
        
        if isinstance(self.state_space, spaces.Box):
            return np.random.uniform(
                low=self.state_space.low,
                high=self.state_space.high,
                size=self.state_space.shape
            )
        elif isinstance(self.state_space, spaces.Discrete):
            return np.array([random.randint(0, self.state_space.n - 1)])
        else:
            return np.random.randn(10)
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """Take step in environment"""
        self.current_step += 1
        
        # Generate next state
        if isinstance(self.state_space, spaces.Box):
            next_state = np.random.uniform(
                low=self.state_space.low,
                high=self.state_space.high,
                size=self.state_space.shape
            )
        else:
            next_state = np.array([random.randint(0, self.state_space.n - 1)])
        
        # Generate reward
        reward = np.random.normal(0, 1)  # Random reward
        
        # Check if done
        done = self.current_step >= self.max_steps_per_episode
        
        info = {'step': self.current_step, 'action': action}
        
        return next_state, reward, done, info

# Integration with Stellar Logic AI
class ReinforcementLearningAIIntegration:
    """Integration layer for reinforcement learning"""
    
    def __init__(self):
        self.rl_platform = ReinforcementLearningPlatform()
        self.active_agents = {}
        
    def deploy_rl_system(self, rl_config: Dict[str, Any]) -> Dict[str, Any]:
        """Deploy reinforcement learning system"""
        print("ğŸ¤– Deploying Reinforcement Learning Platform...")
        
        # Create agents
        agent_configs = rl_config.get('agents', [
            {'algorithm': 'q_learning', 'state_size': 10, 'action_size': 4},
            {'algorithm': 'deep_q_network', 'state_size': 10, 'action_size': 4}
        ])
        
        created_agents = []
        
        for config in agent_configs:
            agent_id = f"{config['algorithm']}_agent_{int(time.time())}"
            
            # Create spaces
            state_space = spaces.Box(low=-10, high=10, shape=(config['state_size'],))
            action_space = spaces.Discrete(config['action_size'])
            
            # Create agent
            create_result = self.rl_platform.create_agent(
                agent_id, config['algorithm'], state_space, action_space
            )
            
            if create_result.get('creation_success'):
                created_agents.append(agent_id)
        
        if not created_agents:
            return {'error': 'No agents created successfully'}
        
        # Create environment
        env_id = f"resource_allocation_env_{int(time.time())}"
        env_result = self.rl_platform.create_environment(
            env_id, 'resource_allocation', state_size=10, action_size=4
        )
        
        if not env_result.get('creation_success'):
            return {'error': 'Environment creation failed'}
        
        # Train agents
        training_results = []
        for agent_id in created_agents:
            train_result = self.rl_platform.train_agent(agent_id, env_id, num_episodes=50)
            if train_result.get('training_success'):
                training_results.append(train_result)
        
        # Evaluate agents
        evaluation_results = []
        for agent_id in created_agents:
            eval_result = self.rl_platform.evaluate_agent(agent_id, env_id, num_episodes=10)
            if eval_result.get('evaluation_success'):
                evaluation_results.append(eval_result)
        
        # Store active RL system
        system_id = f"rl_system_{int(time.time())}"
        self.active_agents[system_id] = {
            'config': rl_config,
            'created_agents': created_agents,
            'environment_id': env_id,
            'training_results': training_results,
            'evaluation_results': evaluation_results,
            'timestamp': time.time()
        }
        
        return {
            'system_id': system_id,
            'deployment_success': True,
            'rl_config': rl_config,
            'created_agents': created_agents,
            'environment_id': env_id,
            'training_results': training_results,
            'evaluation_results': evaluation_results,
            'platform_summary': self.rl_platform.get_platform_summary(),
            'rl_capabilities': self._get_rl_capabilities()
        }
    
    def _get_rl_capabilities(self) -> Dict[str, Any]:
        """Get RL system capabilities"""
        return {
            'supported_algorithms': [
                'q_learning', 'deep_q_network', 'policy_gradient', 
                'actor_critic', 'ppo', 'ddpg', 'multi_agent_dqn'
            ],
            'supported_environments': [
                'grid_world', 'cart_pole', 'financial_trading', 
                'resource_allocation', 'game_strategy', 'autonomous_navigation'
            ],
            'advanced_features': [
                'multi_agent_learning',
                'experience_replay',
                'target_networks',
                'epsilon_greedy_exploration',
                'policy_optimization'
            ],
            'enterprise_applications': [
                'autonomous_decision_making',
                'resource_optimization',
                'strategic_planning',
                'adaptive_control',
                'game_ai'
            ]
        }

# Usage example and testing
if __name__ == "__main__":
    print("ğŸ¤– Initializing Reinforcement Learning Platform...")
    
    # Initialize RL
    rl = ReinforcementLearningAIIntegration()
    
    # Test RL system
    print("\nğŸ® Testing Reinforcement Learning System...")
    rl_config = {
        'agents': [
            {'algorithm': 'q_learning', 'state_size': 8, 'action_size': 4},
            {'algorithm': 'deep_q_network', 'state_size': 8, 'action_size': 4}
        ]
    }
    
    rl_result = rl.deploy_rl_system(rl_config)
    
    print(f"âœ… Deployment success: {rl_result['deployment_success']}")
    print(f"ğŸ¤– System ID: {rl_result['system_id']}")
    print(f"ğŸ® Created agents: {rl_result['created_agents']}")
    print(f"ğŸŒ Environment ID: {rl_result['environment_id']}")
    
    # Show training results
    for result in rl_result['training_results']:
        print(f"ğŸ“ˆ {result['agent_id']}: Avg reward {result['average_reward']:.3f}")
    
    # Show evaluation results
    for result in rl_result['evaluation_results']:
        print(f"ğŸ¯ {result['agent_id']}: Eval reward {result['average_reward']:.3f}")
    
    print("\nğŸš€ Reinforcement Learning Platform Ready!")
    print("ğŸ¤– Autonomous decision-making deployed!")
